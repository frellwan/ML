---
title: "Train and Test Errors for Different Models"
output: 
   html_document:
      toc: true
      toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#### Frederick (Eric) Ellwanger
#### 09/14/2020  

Load the packages needed for this report.  
```{r message=FALSE, warning=FALSE}
mypackages = c("ggplot2", "class")   
tmp = setdiff(mypackages, rownames(installed.packages())) 
if (length(tmp) > 0) install.packages(tmp)

library(ggplot2)
library(class)

```  

This report looks at both the training error as well as the test errors for a known distribution using multiple modelling techniques - Linear regression, quadratic regression, kNN classification, and Bayes rule.  
<br/>
The 2-dimensional data will be generated from a mixture of 10 different bivariate Gaussian distributions with uncorrelated components and different means.  
\[{X \vert Y = k, Z = l \sim \mathcal{N}(m_{kl}, s^2I_2)}\]  

where $k = 0,1, l = 1:10, P(Y = k) = 1/2, and P(Z = 1) = 1/10$. In other words, given $Y=k, X$ follows a mixture ditribution with density function  

\[\frac{1}{10} \sum_{l=1}^{10} (\frac{1}{\sqrt{2\pi s^2}})^2 e^{-\vert\vert{x-m_{kl}}\vert\vert^2/2s^2}\]   

# 1) Genereate the training and test data  
<br/>
In order to sample a data point from a mixture of 10 bivariate Gausian distribution, the following steps are repeated for each sample

     1. Randomly select one of the 10 ceneters  
     2. Generate a sample from the bivariate Gaussian with the center chosen in step 1  
<br/>  

Generate the 10 centers for each of the two (0 or 1) groups.
```{r}
generateCenters = function() {
   csize = 10;       # number of centers
   p = 2;      
   s = 1;   # sd for generating the centers within each class                    
   m1 = matrix(rnorm(csize*p), csize, p)*s + cbind( rep(1,csize), rep(0,csize));
   m0 = matrix(rnorm(csize*p), csize, p)*s + cbind( rep(0,csize), rep(1,csize));
   
   return (list('m1' = m1, 'm0' = m0))
}
```  


Generate the training data and test data all together.
```{r}
generateData = function(s = sqrt(1/5), M, n=100, N=5000) {
    # s  - sd for generating x
    # n  - # of training samples for each class
    # N  - # of test samples for each class
  
    #parameters for the mixture of 10 bivariate normals
    csize = 10
    p = 2
    
    m1 = M$m1
    m0 = M$m0

    # Randomly allocate the n samples for class 1 to the 10 clusters
    id1 = sample(1:csize, n, replace = TRUE);
    # Randomly allocate the n samples for class 0 to the 10 clusters
    id0 = sample(1:csize, n, replace = TRUE);  
    
    traindata = matrix(rnorm(2*n*p), 2*n, p)*s + rbind(m1[id1,], m0[id0,])
    Ytrain = c(rep(1,n), rep(0,n))
    traindata = cbind(traindata, Ytrain)
    colnames(traindata) = c('X1', 'X2', 'Y')
    train_df = as.data.frame(traindata)

    # Randomly allocate the n samples for class 1  to the 10 clusters
    id11 = sample(1:csize, N, replace=TRUE);
    # Randomly allocate the n samples for class 0 to the 10 clusters
    id10 = sample(1:csize, N, replace=TRUE); 
    
    testdata = matrix(rnorm(2*N*p), 2*N, p)*s + rbind(m1[id11,], m0[id10,])
    Ytest = c(rep(1,N), rep(0,N))
    testdata = cbind(testdata, Ytest)
    colnames(testdata) = c('X1', 'X2', 'Y')
    test_df = as.data.frame(testdata)
    
    return(list('train' = train_df, 'test' = test_df, 'M' = M, 's' = s))
}
```


# 2) Classification based on linear regression  

Fit a linear regression model on the training data and use cut-off value 0.5 to transform numerical outcomes to binary outcomes. Return the misclassification rates of a particular pair of training/test datasets.
```{r}
#Fit a linear model to the data
fit_linear = function(df) {
    #Fit a linear model using the training data
    trainModel = lm(Y ~ X1 + X2, data = df$train)
  
    #Find fitted values for training data and calculate the classification error
    Ytrain_pred = as.numeric(trainModel$fitted > 0.5)
    trainErr = mean(df$train$Y !=  Ytrain_pred) 
    
    #Find predicted values on test data on calculate the classification error
    Ytest_pred = predict(trainModel, newdata = df$test)
    Ytest = as.numeric(Ytest_pred > 0.5)
    testErr = mean(df$test$Y !=  Ytest)

    return(list('trainErr' = trainErr, 'testErr' = testErr))
}
```


# 3) Classification based on quadratic regression  

Fit a quardratic regression model on the training data and use cut-off value 0.5 to transform numerical outcomes to binary outcomes. Return the misclassification rates of a particular pair of training/test datasets.
```{r}
fit_poly2 = function(df) {
    #Fit a degree 2 polynomial model to the training data
    trainModelPoly2 = lm(Y ~ X1 + X2 + I(X1 * X2) + I(X1^2) + I(X2^2), data = df$train)
    
    #Find fitted values for training data and calculate the classification error
    Ytrain_pred = as.numeric(trainModelPoly2$fitted > 0.5)
    trainErr = mean(df$train$Y !=  Ytrain_pred)
    
    #Find fitted values for test data and calculate the classification error
    Ytest_pred = predict(trainModelPoly2, newdata = df$test[,c('X1', 'X2')])
    Ytest = as.numeric(Ytest_pred > 0.5)
    testErr = mean(df$test$Y !=  Ytest)
    
    return(list('trainErr' = trainErr, 'testErr' = testErr))
}
```  



# 4) Classification based on KNN with K chosen by 10 Fold Cross Validation.  

Function to find optimal k for kNN model based on 10 fold cross validation using the training data. Also calculate the error of the test data based on this optimal k. Return the best k value, the misclassification rates of the training and test datasets using that k. 
```{r}
#Fit kNN model
cvKNNAveErrorRate = function(k, df, foldSize, foldNum = 10) {

    error = 0
    
    #Calculate error for each fold using train data and test data
    for (runId in 1:foldNum) {
        testSetIndex = ((runId - 1) * foldSize + 1):
            (ifelse(runId == foldNum, nrow(df), runId * foldSize))

        trainX = df[-testSetIndex, c("X1", "X2")]
        trainY = as.factor(df[-testSetIndex, "Y"])
        testX = df[testSetIndex, c("X1", "X2")]
        testY = as.factor(df[testSetIndex, "Y"])
        predictY = knn(trainX, testX, trainY, k)
        error = error + sum(predictY != testY)
    }
    
    #Calculate overall cross validated error rate
    error = error/nrow(df)
    
    return(error)
}


cvKNN = function(df, foldNum = 10) {
    
    #Randomize train data for CV since dataSet is not really random
    myIndex = sample(1:nrow(df$train))  
    randomTrain = df$train[myIndex, ]

    #calculate foldsize and generate a vector k values to use to find best k value
    foldSize = floor(nrow(df$train)/foldNum)
    KVector = seq(1, (nrow(df$train) - foldSize), 2)

    #Find best k using cross validation
    cvKNNAveErrorRates = sapply(KVector, cvKNNAveErrorRate, randomTrain, 
        foldSize, foldNum)
    result = list()
    result$bestK = max(KVector[cvKNNAveErrorRates == min(cvKNNAveErrorRates)])
    
    #Now having found best k, get error rate for entire train set
    trainY = as.factor(randomTrain[, "Y"])
    predictYtrain = knn(randomTrain[,c("X1", "X2")], randomTrain[,c("X1", "X2")], 
                   trainY, result$bestK)
    result$trainErr = mean(predictYtrain != trainY)
    
    #Find Error rate using test data with best k from training set
    testY = as.factor(df$test[, "Y"])
    predictYtest = knn(randomTrain[,c("X1", "X2")], df$test[,c("X1", "X2")], 
                   trainY, result$bestK)
    result$testErr = mean(predictYtest != testY)
    
    return(result)
}
```  



# 5) Classification based on the Bayes Rule  

Calculate the Bayes misclassification rates for both the training data and the test data.
```{r}
mixnorm = function(x, M, s){
  ## return the density ratio for a point x, where each 
  ## density is a mixture of normal with 10 components
  sum(exp(-apply((t(M$m1)-x)^2, 2, sum)*(1/(2*s^2))))/sum(exp(-apply((t(M$m0)-x)^2, 2, sum)*(1/(2*s^2))))
}


bayesRule = function(df) {

    Ytrain_pred_Bayes = apply(df$train[c('X1', 'X2')], 1, mixnorm, df$M, df$s)
    Ytrain_pred = as.numeric(Ytrain_pred_Bayes > 1);
    trainErr = mean(df$train$Y !=  Ytrain_pred)
    
    Ytest_pred_Bayes = apply(df$test[c('X1', 'X2')], 1, mixnorm, df$M, df$s)
    Ytest_pred = as.numeric(Ytest_pred_Bayes > 1);
    testErr = mean(df$test$Y !=  Ytest_pred)
    
    return(list('trainErr' = trainErr, 'testErr' = testErr))
}
```  


# 6) Run a simulation 20 times and compare the performance for all methods.
```{r}
#Last 4 digits of my UIN - 6950
set.seed(6950)

#Initialize the error vectors to empty vectors
linFitTrainErr  = c(rep(0,20))
linFitTestErr   = c(rep(0,20))
polyFitTrainErr = c(rep(0,20))
polyFitTestErr  = c(rep(0,20))
kNNCVTrainErr   = c(rep(0,20))
kNNCVTestErr    = c(rep(0,20))
kNNCVBestK      = c(rep(0,20))
bayesTrainErr   = c(rep(0,20))
bayesTestErr    = c(rep(0,20))

#Start 20 simulations
M = generateCenters()
for (i in 1:20) {
    #Generate the data at the beginning of each simulation 
    data = generateData(M = M)
    
    #Get linear model errors
    errors = fit_linear(data)
    linFitTrainErr[i] = errors$trainErr
    linFitTestErr[i]  = errors$testErr
    
    #get polynomial model errors
    errors = fit_poly2(data)
    polyFitTrainErr[i] = errors$trainErr
    polyFitTestErr[i]  = errors$testErr
    
    #get 10 fold cross validated kNN errors and best k value
    errors = cvKNN(data)
    kNNCVTrainErr[i]   = errors$trainErr
    kNNCVTestErr[i]    = errors$testErr
    kNNCVBestK[i]      = errors$bestK
    
    #Calculate the bayes errors for train and test data
    errors = bayesRule(data)
    bayesTrainErr[i]   = errors$trainErr
    bayesTestErr[i]    = errors$testErr     
}


```  

  
### Compare the performance of all of the above classification methods  
```{r, fig.width=9,fig.height=5}
ModelType = c(rep("LinearReg",20))
Set = c(rep("TrainErr",20))
df1  = data.frame(ModelType,Set, 'ErrorRate' = linFitTrainErr)

ModelType = c(rep("LinearReg",20))
Set = c(rep("TestErr",20))
df1  = rbind(df1, data.frame(ModelType,Set,'ErrorRate' = linFitTestErr))

ModelType = c(rep("PolyReg",20))
Set = c(rep("TrainErr",20))
df1  = rbind(df1, data.frame(ModelType,Set,'ErrorRate' = polyFitTrainErr))

ModelType = c(rep("PolyReg",20))
Set = c(rep("TestErr",20))
df1  = rbind(df1, data.frame(ModelType,Set, 'ErrorRate' = polyFitTestErr))

ModelType = c(rep("kNNCV",20))
Set = c(rep("TrainErr",20))
df1  = rbind(df1, data.frame(ModelType,Set,'ErrorRate' = kNNCVTrainErr))

ModelType = c(rep("kNNCV",20))
Set = c(rep("TestErr",20))
df1  = rbind(df1, data.frame(ModelType,Set,'ErrorRate' = kNNCVTestErr))

ModelType = c(rep("Bayes",20))
Set = c(rep("TrainErr",20))
df1  = rbind(df1, data.frame(ModelType,Set,'ErrorRate' = bayesTrainErr))

ModelType = c(rep("Bayes",20))
Set = c(rep("TestErr",20))
df1  = rbind(df1, data.frame(ModelType,Set,'ErrorRate' = bayesTestErr))

#Set colors for the different errors
group.colors = c(TestErr = "orange", TrainErr = "steelblue")

#plot the boxplots of errors
ggplot(data = df1, aes(x=ModelType, y=ErrorRate)) + 
    geom_boxplot(aes(fill=Set)) +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title="Error Rate v Model Type for Simulated Data", x ="Model Type", y = "Error Rate") +
    scale_fill_manual(values=group.colors)
```
  
### Bar Chart of the best k chosen by kNN with 10 Fold Cross Validation  
```{r}
ggplot(data=data.frame(kNNCVBestK), aes(x=1:length(kNNCVBestK), y=kNNCVBestK)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_bar(stat="identity", fill="steelblue") +
  labs(title="Best k for kNN using 10-Fold Cross Validation", x ="Simulation #", y = "Best k")
```
  
Report the mean of selected K values.  
```{r}
mean(kNNCVBestK)
```  

Report the standard error of selected K values.  
```{r}
sd(kNNCVBestK)
```

