# Statistacal Learning
This course provides an introduction to modern techniques for statistical analysis of complex and massive data. Examples of these are model selection for regression/classification, nonparametric models including splines and kernel models, regularization, model ensemble, recommender system, and clustering analysis. Learn applications as well as computation and theoretical foundations. The coding assignments and projects will use the R programming language.

## Coding 1 - Classification algorithm evaluation
This coding evaluates the performance of various classification models — linear regression, quadratic regression, k-nearest neighbors (kNN), and Bayes rule — on a synthetic dataset generated from mixtures of bivariate Gaussian distributions. The dataset is split into training and test sets, and the misclassification rates for each model are calculated and compared. The kNN model's optimal k is determined via 10-fold cross-validation. The script iterates this process 20 times to assess model robustness, presenting the results through detailed plots and statistical summaries.

## Coding 2 - Lasso Regression
This coding demonstrates the implementation of Lasso regression using the Coordinate Descent algorithm on the Boston housing dataset. It includes data preparation, transformation, and a custom function (MyLasso) to compute Lasso coefficients. The results are validated by comparing the custom implementation with the glmnet package, ensuring accuracy. Key steps involve loading data, transforming variables, and iteratively updating coefficients. The provided R code highlights efficient computation and comparison techniques for Lasso regression.

## Coding 3 - Local Regression 
This coding explores the application of Loess (Local Regression) for non-parametric smoothing and prediction. The primary functions include lo.lev to compute the diagonal entries of the smoother matrix, onestep_CV to fit a Loess model and calculate Leave-One-Out Cross-Validation (LOO-CV) and Generalized Cross-Validation (GCV) errors, and myCV to evaluate these metrics over a range of span values. The script tests these functions on data loaded from a CSV file, identifying the optimal span that minimizes RSS for both GCV and LOO-CV, and plots the fitted curve using the optimal span.

## Coding 4 - Expectation Maximization
This coding demonstrates the implementation of the Expectation Maximization (EM) algorithm for estimating parameters of a Gaussian Mixture Model (GMM). It starts with loading necessary packages and introduces the EM algorithm's theory, which iteratively improves parameter estimates for mixtures of Gaussian distributions. The code includes functions for the E-step (computing posterior probabilities) and M-step (updating parameters) and an myEM function to iteratively apply these steps. The algorithm is tested using the faithful dataset, with initial parameters derived from the mclust package for comparison.

## Project 1 - Home Price Predictions
This project analyzes and models the AMES Housing dataset, which contains 2,930 records and 83 variables describing house characteristics. The primary goal is to predict house sale prices using various modeling techniques. Initial preprocessing involved handles missing values and converting categorical variables into binary columns. Two main modeling approaches were employed: boosting tree methods using the xgboost package, and elastic net regression using the glmnet package. Both models' performances are evaluated across multiple train/test splits. The project concludes with a comparison of the error rates and observations on model performance.

## Project 2 - Weekly Sales Predictions
This project processes and evaluates a machine learning model for predicting weekly sales using time-series data. It first loads necessary libraries and sources an external script containing the prediction function. The script reads training and test datasets and iteratively performs 10-fold cross-validation. For each fold, it reads specific fold data, generates predictions, and computes the Weighted Mean Absolute Error (WMAE) based on holiday weighting. The script outputs the WMAE for each fold and the mean WMAE, alongside the total execution time.

## Project 3 - Movie Review Sentiment
This project focuses on building a binary classifier to predict whether a movie review is positive or negative using a dataset of 50,000 IMDb reviews. The dataset is divided into five splits of training and test data, with reviews rated from 1-4 as negative and 7-10 as positive. A logistic regression model with a ridge penalty term is employed, utilizing the bag-of-words approach and the text2vec package in R. The model achieves an AUC greater than 0.96 for all splits, demonstrating high accuracy in classifying review sentiments.
 
## Project 4 - Movie Recommender System
This project is designed to build a recommender system using the Movielens dataset. It begins by setting up the environment, installing and loading necessary packages such as recommenderlab and tidyverse. The script then reads in the data, including ratings and movie metadata, and performs exploratory data analysis (EDA) to understand the distribution of ratings and genres. The script addresses user rating biases through normalization and visualizes the distribution of ratings per user and movie. Two content-based recommendation systems are implemented: one for highly-rated movies and another for popular movies within specific genres. Finally, collaborative filtering models (User-Based and Item-Based) and other recommendation algorithms are trained and evaluated using cross-validation to predict movie ratings.
