# Statistacal Learning
This course provides an introduction to modern techniques for statistical analysis of complex and massive data. Examples of these are model selection for regression/classification, nonparametric models including splines and kernel models, regularization, model ensemble, recommender system, and clustering analysis. Learn applications as well as computation and theoretical foundations. The coding assignments and projects will use the R programming language.

## Coding 1 - Classification algorithm evaluation
This coding evaluates the performance of various classification models — linear regression, quadratic regression, k-nearest neighbors (kNN), and Bayes rule — on a synthetic dataset generated from mixtures of bivariate Gaussian distributions. The dataset is split into training and test sets, and the misclassification rates for each model are calculated and compared. The kNN model's optimal k is determined via 10-fold cross-validation. The script iterates this process 20 times to assess model robustness, presenting the results through detailed plots and statistical summaries.

## Coding 2 - Lasso Regression
This coding demonstrates the implementation of Lasso regression using the Coordinate Descent algorithm on the Boston housing dataset. It includes data preparation, transformation, and a custom function (MyLasso) to compute Lasso coefficients. The results are validated by comparing the custom implementation with the glmnet package, ensuring accuracy. Key steps involve loading data, transforming variables, and iteratively updating coefficients. The provided R code highlights efficient computation and comparison techniques for Lasso regression.

## Coding 3 - Local Regression 
This coding explores the application of Loess (Local Regression) for non-parametric smoothing and prediction. The primary functions include lo.lev to compute the diagonal entries of the smoother matrix, onestep_CV to fit a Loess model and calculate Leave-One-Out Cross-Validation (LOO-CV) and Generalized Cross-Validation (GCV) errors, and myCV to evaluate these metrics over a range of span values. The script tests these functions on data loaded from a CSV file, identifying the optimal span that minimizes RSS for both GCV and LOO-CV, and plots the fitted curve using the optimal span.

## Coding 4 - Expectation Maximization
This coding demonstrates the implementation of the Expectation Maximization (EM) algorithm for estimating parameters of a Gaussian Mixture Model (GMM). It starts with loading necessary packages and introduces the EM algorithm's theory, which iteratively improves parameter estimates for mixtures of Gaussian distributions. The code includes functions for the E-step (computing posterior probabilities) and M-step (updating parameters) and an myEM function to iteratively apply these steps. The algorithm is tested using the faithful dataset, with initial parameters derived from the mclust package for comparison.
